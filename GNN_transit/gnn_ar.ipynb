{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set CUDA devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from functools import partial\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Primary device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    \"\"\"Dictionary wrapper to access keys as attributes.\"\"\"\n",
    "    def __getattr__(self, key):\n",
    "        return self.get(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_vocabulary(num_nodes=50):\n",
    "    \"\"\"\n",
    "    Build a canonical ordering of edges (i < j) for up to num_nodes=50.\n",
    "    Also add special tokens: <PAD>, <START>, <END>\n",
    "    Returns:\n",
    "        edge_to_token: dict (i,j) -> token_id\n",
    "        token_to_edge: dict token_id -> (i,j)\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i+1, num_nodes):\n",
    "            edges.append((i, j))\n",
    "    # Sort them lexicographically => canonical ordering\n",
    "    edges.sort(key=lambda x: (x[0], x[1]))\n",
    "    edge_to_token = {}\n",
    "    token_to_edge = {}\n",
    "    for idx, (i, j) in enumerate(edges):\n",
    "        edge_to_token[(i, j)] = idx\n",
    "        token_to_edge[idx] = (i, j)\n",
    "    \n",
    "    # Special Tokens\n",
    "    PAD_TOKEN = len(edges)\n",
    "    START_TOKEN = len(edges) + 1\n",
    "    END_TOKEN = len(edges) + 2\n",
    "    \n",
    "    edge_to_token['<PAD>'] = PAD_TOKEN\n",
    "    edge_to_token['<START>'] = START_TOKEN\n",
    "    edge_to_token['<END>'] = END_TOKEN\n",
    "    \n",
    "    token_to_edge[PAD_TOKEN] = '<PAD>'\n",
    "    token_to_edge[START_TOKEN] = '<START>'\n",
    "    token_to_edge[END_TOKEN] = '<END>'\n",
    "    \n",
    "    return edge_to_token, token_to_edge\n",
    "\n",
    "\n",
    "temp_edge2tok, temp_tok2edge = build_global_vocabulary(5)\n",
    "assert len(temp_edge2tok) == 13, f\"5 nodes => 10 edges + 3 special tokens, got {len(temp_edge2tok)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects lines: x1 y1 x2 y2 ... xN yN output 1 2 3 ... N 1\n",
    "    We do:\n",
    "      - parse node coords\n",
    "      - parse solution tour\n",
    "      - find top-down edges crossing y-mid\n",
    "      - find left-right edges crossing x-mid\n",
    "      - produce a single sequence of tokens:\n",
    "         [<START>, topDownEdgeTokens..., leftRightEdgeTokens..., <END>]\n",
    "      - also produce node coords, x_edges, x_edges_values for GCN\n",
    "      - precompute an 'edge_rank' array for each sample, used in\n",
    "        generating edge context.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath, num_nodes=50, edge_to_token=None, token_to_edge=None,\n",
    "                 num_neighbors=-1, device='cpu', voc_size=None):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.edge_to_token = edge_to_token  # global edge->token mapping\n",
    "        self.token_to_edge = token_to_edge  # reverse mapping\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.device = device\n",
    "\n",
    "        # If not provided, infer from dictionary (minus special tokens)\n",
    "        if voc_size is None:\n",
    "            # e.g. len(edge_to_token) might already include special tokens\n",
    "            voc_size = len(edge_to_token)\n",
    "        self.voc_size = voc_size\n",
    "\n",
    "        with open(filepath, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        self.lines = lines\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # parse line\n",
    "        line = self.lines[idx].strip().split()\n",
    "        if len(line) < 2*self.num_nodes:\n",
    "            raise ValueError(f\"Line {idx} too short to have {2*self.num_nodes} coords.\")\n",
    "\n",
    "        coords = np.array(line[:2*self.num_nodes], dtype=np.float32).reshape(self.num_nodes, 2)\n",
    "\n",
    "        coords_swapped = coords[:, [1, 0]]\n",
    "        \n",
    "        # solution parse\n",
    "        try:\n",
    "            output_idx = line.index('output')\n",
    "        except ValueError:\n",
    "            raise ValueError(\"No 'output' in line.\")\n",
    "        # parse solution nodes (1-based => subtract 1)\n",
    "        sol_nodes = [int(x)-1 for x in line[output_idx+1:] if x.isdigit()]\n",
    "        if len(sol_nodes) != self.num_nodes+1 or sol_nodes[0] != sol_nodes[-1]:\n",
    "            raise ValueError(\"Solution tour not matching expected length or not cyclic.\")\n",
    "        sol_nodes = sol_nodes[:-1]  # remove the repeat\n",
    "\n",
    "        # compute midpoints\n",
    "        x_mid = coords[:, 0].mean()\n",
    "        y_mid = coords[:, 1].mean()\n",
    "\n",
    "        # define quadrant\n",
    "        quadrants = np.ones(self.num_nodes, dtype=int)\n",
    "        quadrants[coords[:, 0] < x_mid] = 2\n",
    "        quadrants[coords[:, 1] < y_mid] = 4\n",
    "        quadrants[(coords[:, 0] < x_mid) & (coords[:, 1] < y_mid)] = 3\n",
    "\n",
    "        # collect top-down edges\n",
    "        topDownEdgePairs = []\n",
    "        for i in range(self.num_nodes):\n",
    "            j = (i + 1) % self.num_nodes\n",
    "            q_i = quadrants[sol_nodes[i]]\n",
    "            q_j = quadrants[sol_nodes[j]]\n",
    "            if ((q_i in [1, 2] and q_j in [3, 4]) or (q_j in [1, 2] and q_i in [3, 4])):\n",
    "                topDownEdgePairs.append((min(sol_nodes[i], sol_nodes[j]),\n",
    "                                         max(sol_nodes[i], sol_nodes[j])))\n",
    "\n",
    "        # collect left-right edges\n",
    "        leftRightEdgePairs = []\n",
    "        for i in range(self.num_nodes):\n",
    "            j = (i + 1) % self.num_nodes\n",
    "            q_i = quadrants[sol_nodes[i]]\n",
    "            q_j = quadrants[sol_nodes[j]]\n",
    "            if ((q_i in [1, 4] and q_j in [2, 3]) or (q_j in [1, 4] and q_i in [2, 3])):\n",
    "                leftRightEdgePairs.append((min(sol_nodes[i], sol_nodes[j]),\n",
    "                                           max(sol_nodes[i], sol_nodes[j])))\n",
    "\n",
    "        # canonical sort & tokenize\n",
    "        def sort_and_tokenize(edgePairs):\n",
    "            edgePairs = list(set(edgePairs))\n",
    "            edgePairs.sort(key=lambda x: (x[0], x[1]))\n",
    "            tokens = [self.edge_to_token[(i, j)] for (i, j) in edgePairs]\n",
    "            return tokens\n",
    "\n",
    "        # special tokens\n",
    "        START_TOKEN = self.edge_to_token['<START>']\n",
    "        END_TOKEN = self.edge_to_token['<END>']\n",
    "\n",
    "        topDownTokens = [START_TOKEN] + sort_and_tokenize(topDownEdgePairs) + [END_TOKEN]\n",
    "        leftRightTokens = [START_TOKEN] + sort_and_tokenize(leftRightEdgePairs) + [END_TOKEN]\n",
    "\n",
    "        \n",
    "\n",
    "        # final sequence = [START_TOKEN] + topDownTokens + leftRightTokens + [END_TOKEN]\n",
    "        final_token_seq = [START_TOKEN] + topDownTokens + leftRightTokens + [END_TOKEN]\n",
    "\n",
    "        # distance matrix\n",
    "        W_val = squareform(pdist(coords, metric='euclidean'))  # (N, N)\n",
    "\n",
    "        # adjacency matrix\n",
    "        if self.num_neighbors == -1:\n",
    "            W = np.ones((self.num_nodes, self.num_nodes), dtype=np.float32)\n",
    "        else:\n",
    "            W = np.zeros((self.num_nodes, self.num_nodes), dtype=np.float32)\n",
    "            knns = np.argpartition(W_val, self.num_neighbors, axis=1)[:, :self.num_neighbors]\n",
    "            W[np.arange(self.num_nodes)[:, None], knns] = 1\n",
    "        np.fill_diagonal(W, 2)  # self-connections\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # Precompute 'edge_rank' for each edge token in [0..voc_size-3]\n",
    "        # (exclude <PAD>, <START>, <END>), store in sample.\n",
    "        # ranking rule:\n",
    "        #   - If W(i,j) != 1 => rank_val = -999999\n",
    "        #   - Else rank_val = negative absolute difference => -(abs(meanY - x_mid))\n",
    "        #     (So that \"closer to x_mid\" => higher rank_val)\n",
    "        # ---------------------------------------------------------\n",
    "        edge_rank = np.full((self.voc_size,), -999999, dtype=np.float32)\n",
    "\n",
    "        for token_id in range(self.voc_size - 3):\n",
    "            # skip the special tokens indices at the end\n",
    "            if token_id not in self.token_to_edge:\n",
    "                continue\n",
    "            i, j = self.token_to_edge[token_id]\n",
    "            # Check adjacency: we treat it as \"valid\" if W(i, j) == 1 or W(j, i) == 1\n",
    "            if i < self.num_nodes and j < self.num_nodes:\n",
    "                if (W[i, j] == 1) or (W[j, i] == 1):\n",
    "                    meanX = (coords[i, 0] + coords[j, 0]) / 2.0\n",
    "                    diff = abs(meanX - x_mid)\n",
    "                    edge_rank[token_id] = -diff\n",
    "\n",
    "        # to torch\n",
    "        sample = {\n",
    "            'coords': torch.tensor(coords, dtype=torch.float, device=self.device),       # (N,2)\n",
    "            'coords_swapped': torch.tensor(coords_swapped, dtype=torch.float, device=self.device),       # (N,2)\n",
    "            'x_edges': torch.tensor(W, dtype=torch.long, device=self.device),           # (N,N)\n",
    "            'x_edges_values': torch.tensor(W_val, dtype=torch.float, device=self.device),  # (N,N)\n",
    "            'topDownTokens': torch.tensor(topDownTokens, dtype=torch.long, device=self.device),\n",
    "            'leftRightTokens': torch.tensor(leftRightTokens, dtype=torch.long, device=self.device),\n",
    "            'token_seq': torch.tensor(final_token_seq, dtype=torch.long, device=self.device),\n",
    "            'edge_rank': torch.tensor(edge_rank, dtype=torch.float, device=self.device), # (voc_size,)\n",
    "            'x_mid': torch.tensor(x_mid, dtype=torch.float, device=self.device),\n",
    "            'y_mid': torch.tensor(y_mid, dtype=torch.float, device=self.device),\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, edge_to_token):\n",
    "    \"\"\"\n",
    "    Custom collate function to pad sequences to the same length.\n",
    "    \"\"\"\n",
    "    coords = torch.stack([sample['coords'] for sample in batch], dim=0)  # (B, N, 2)\n",
    "    coords_swapped = torch.stack([sample['coords_swapped'] for sample in batch], dim=0)  # (B, N, 2)\n",
    "    x_edges = torch.stack([sample['x_edges'] for sample in batch], dim=0)  # (B, N, N)\n",
    "    x_edges_values = torch.stack([sample['x_edges_values'] for sample in batch], dim=0)  # (B, N, N)\n",
    "    edge_rank = torch.stack([sample['edge_rank'] for sample in batch], dim=0)  # (B, voc_size)\n",
    "    x_mid = torch.stack([sample['x_mid'] for sample in batch], dim=0)  # (B,)\n",
    "    y_mid = torch.stack([sample['y_mid'] for sample in batch], dim=0)  # (B,)\n",
    "    \n",
    "\n",
    "    # Determine padding token ID\n",
    "    PAD_TOKEN = edge_to_token['<PAD>']\n",
    "    \n",
    "    # Gather all token sequences\n",
    "    token_seqs = [sample['token_seq'] for sample in batch]\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    token_seqs_padded = pad_sequence(token_seqs, batch_first=True, padding_value=PAD_TOKEN)  # (B, L)\n",
    "\n",
    "    # Gather all token sequences\n",
    "    td_token_seqs = [sample['topDownTokens'] for sample in batch]\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    td_token_seqs_padded = pad_sequence(td_token_seqs, batch_first=True, padding_value=PAD_TOKEN)  # (B, L1)\n",
    "\n",
    "    # Gather all token sequences\n",
    "    lr_token_seqs = [sample['leftRightTokens'] for sample in batch]\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    lr_token_seqs_padded = pad_sequence(lr_token_seqs, batch_first=True, padding_value=PAD_TOKEN)  # (B, L2)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'coords': coords,\n",
    "        'coords_swapped': coords_swapped,\n",
    "        'x_edges': x_edges,\n",
    "        'x_edges_values': x_edges_values,\n",
    "        'token_seq': token_seqs_padded,  # (B, L) legacy\n",
    "        'topDownTokens': td_token_seqs_padded,  # (B, L1),\n",
    "        'leftRightTokens': lr_token_seqs_padded,  # (B, L2)\n",
    "        'edge_rank': edge_rank,\n",
    "        'x_mid': x_mid,\n",
    "        'y_mid': y_mid,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 50, 2]),\n",
       " torch.Size([32, 50, 50]),\n",
       " torch.Size([32, 50, 50]),\n",
       " torch.Size([32, 8]),\n",
       " torch.Size([32, 8]),\n",
       " torch.Size([32, 1228]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_edge2tok, temp_tok2edge = build_global_vocabulary(50)\n",
    "dataset_dummy = TSPDataset(filepath=\"tsp-data/tsp50_test_concorde.txt\", num_nodes=50, edge_to_token=temp_edge2tok, token_to_edge=temp_tok2edge)\n",
    "collate_fn = partial(collate_fn, edge_to_token=temp_edge2tok)\n",
    "dataloader_dummy = DataLoader(dataset_dummy, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "data_dict = next(iter(dataloader_dummy))\n",
    "data_dict['coords'].size(), data_dict['x_edges'].size(), data_dict['x_edges_values'].size(), data_dict['topDownTokens'].size(), data_dict['leftRightTokens'].size(), data_dict['edge_rank'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(batch, edge_to_token, token_to_edge, N=2):\n",
    "    \"\"\"\n",
    "    Visualize a single sample from the batch.\n",
    "\n",
    "    Args:\n",
    "        batch (dict): A batch from DataLoader.\n",
    "        edge_to_token (dict): Mapping from edge tuples to token IDs.\n",
    "        token_to_edge (dict): Mapping from token IDs to edge tuples.\n",
    "        edge_rank (torch.Tensor): Tensor of shape (B, vocab_size) with edge ranks.\n",
    "        N (int): Number of top edges to highlight.\n",
    "    \"\"\"\n",
    "    # Select the first sample in the batch\n",
    "    sample = {k: v[0] for k, v in batch.items()}\n",
    "    \n",
    "    coords = sample['coords'].cpu().numpy()                # (N, 2)\n",
    "    x_edges = sample['x_edges'].cpu().numpy()              # (N, N)\n",
    "    topDownTokens = sample['topDownTokens'].cpu().numpy()  # List of tokens\n",
    "    leftRightTokens = sample['leftRightTokens'].cpu().numpy()  # List of tokens\n",
    "    edge_rank_sample = sample['edge_rank'].cpu().numpy()   # (vocab_size,)\n",
    "    x_mid = sample['x_mid'].cpu().numpy()\n",
    "    y_mid = sample['y_mid'].cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Plot all nodes\n",
    "    plt.scatter(coords[:, 0], coords[:, 1], color='blue', label='Nodes')\n",
    "    \n",
    "    '''\n",
    "    # Plot all valid x_edges as thin grey lines\n",
    "    for i in range(len(coords)):\n",
    "        for j in range(i + 1, len(coords)):\n",
    "            if x_edges[i, j] == 1:\n",
    "                plt.plot([coords[i, 0], coords[j, 0]],\n",
    "                         [coords[i, 1], coords[j, 1]],\n",
    "                         color='grey', linewidth=0.5)\n",
    "    '''\n",
    "    \n",
    "    # Plot top-down edges (excluding special tokens)\n",
    "    topDownTokens_trimmed = [t for t in topDownTokens if t not in [edge_to_token['<PAD>'], edge_to_token['<START>'], edge_to_token['<END>']]]\n",
    "    for token in topDownTokens_trimmed:\n",
    "        if token in token_to_edge:\n",
    "            i, j = token_to_edge[token]\n",
    "            plt.plot([coords[i, 0], coords[j, 0]],\n",
    "                     [coords[i, 1], coords[j, 1]],\n",
    "                     color='red', linewidth=2, label='Top-Down Edge' if token == topDownTokens_trimmed[0] else \"\")\n",
    "    \n",
    "    # Plot left-right edges (excluding special tokens)\n",
    "    leftRightTokens_trimmed = [t for t in leftRightTokens if t not in [edge_to_token['<PAD>'], edge_to_token['<START>'], edge_to_token['<END>']]]\n",
    "    for token in leftRightTokens_trimmed:\n",
    "        if token in token_to_edge:\n",
    "            i, j = token_to_edge[token]\n",
    "            plt.plot([coords[i, 0], coords[j, 0]],\n",
    "                     [coords[i, 1], coords[j, 1]],\n",
    "                     color='green', linewidth=2, label='Left-Right Edge' if token == leftRightTokens_trimmed[0] else \"\")\n",
    "    \n",
    "    # Highlight top N edges based on edge_rank\n",
    "    if len(topDownTokens_trimmed) > 0:\n",
    "        # Get ranks for valid topDownTokens\n",
    "        valid_tokens = torch.tensor(topDownTokens_trimmed, dtype=torch.long)\n",
    "        ranks = edge_rank_sample[topDownTokens_trimmed]\n",
    "        # Get top N tokens with highest rank (least negative)\n",
    "        topN_indices = torch.topk(torch.tensor(ranks), N, largest=True).indices\n",
    "        topN_tokens = valid_tokens[topN_indices].cpu().numpy()\n",
    "        for token in topN_tokens:\n",
    "            if token in token_to_edge:\n",
    "                i, j = token_to_edge[token]\n",
    "                plt.plot([coords[i, 0], coords[j, 0]],\n",
    "                         [coords[i, 1], coords[j, 1]],\n",
    "                         color='orange', linewidth=3, label='Top-N Edge' if token == topN_tokens[0] else \"\")\n",
    "    \n",
    "    # Plot x_mid and y_mid\n",
    "    plt.axvline(x=x_mid, color='purple', linestyle='--', label='x_mid')\n",
    "    plt.axhline(y=y_mid, color='brown', linestyle='--', label='y_mid')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(\"TSP Sample Visualization\")\n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edge2tok, tok2edge = build_global_vocabulary(50) \n",
    "dummy_dataset = TSPDataset(\"tsp-data/tsp50_micro.txt\", num_nodes=50, edge_to_token=edge2tok, token_to_edge=tok2edge, device=device)\n",
    "collate_fn = partial(collate_fn, edge_to_token=edge2tok)\n",
    "dummy_loader = DataLoader(dummy_dataset, batch_size=1, shuffle=True, drop_last=False, collate_fn=collate_fn)\n",
    "\n",
    "sample = next(iter(dummy_loader))\n",
    "visualize_sample(sample, edge2tok, tok2edge, N=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Batch Normalization layers\n",
    "class BatchNormNode(nn.Module):\n",
    "    \"\"\"Batch normalization for node features.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BatchNormNode, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim, track_running_stats=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, V, H)\n",
    "        x = x.transpose(1, 2).contiguous()  # (B, H, V)\n",
    "        x = self.batch_norm(x)  # Apply BatchNorm1d on H\n",
    "        x = x.transpose(1, 2).contiguous()  # (B, V, H)\n",
    "        return x\n",
    "\n",
    "class BatchNormEdge(nn.Module):\n",
    "    \"\"\"Batch normalization for edge features.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BatchNormEdge, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(hidden_dim, track_running_stats=False)\n",
    "\n",
    "    def forward(self, e):\n",
    "        # e: (B, V, V, H)\n",
    "        e = e.permute(0, 3, 1, 2).contiguous()  # (B, H, V, V)\n",
    "        e = self.batch_norm(e)  # Apply BatchNorm2d on H\n",
    "        e = e.permute(0, 2, 3, 1).contiguous()  # (B, V, V, H)\n",
    "        return e\n",
    "\n",
    "# Define NodeFeatures\n",
    "class NodeFeatures(nn.Module):\n",
    "    \"\"\"Convnet features for nodes.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, aggregation=\"mean\"):\n",
    "        super(NodeFeatures, self).__init__()\n",
    "        self.aggregation = aggregation\n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_gate):\n",
    "        # x: (B, V, H)\n",
    "        # edge_gate: (B, V, V, H)\n",
    "        Ux = self.U(x)  # (B, V, H)\n",
    "        Vx = self.V(x).unsqueeze(1)  # (B, 1, V, H)\n",
    "        gateVx = edge_gate * Vx  # (B, V, V, H)\n",
    "        if self.aggregation == \"mean\":\n",
    "            aggregation = gateVx.sum(dim=2) / (edge_gate.sum(dim=2) + 1e-20)  # (B, V, H)\n",
    "        else:\n",
    "            aggregation = gateVx.sum(dim=2)  # (B, V, H)\n",
    "        return Ux + aggregation  # (B, V, H)\n",
    "\n",
    "# Define EdgeFeatures\n",
    "class EdgeFeatures(nn.Module):\n",
    "    \"\"\"Convnet features for edges.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(EdgeFeatures, self).__init__()\n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        # x: (B, V, H)\n",
    "        # e: (B, V, V, H)\n",
    "        Ue = self.U(e)  # (B, V, V, H)\n",
    "        Vx_i = self.V(x).unsqueeze(2)  # (B, V, 1, H)\n",
    "        Vx_j = self.V(x).unsqueeze(1)  # (B, 1, V, H)\n",
    "        return Ue + Vx_i + Vx_j  # (B, V, V, H)\n",
    "\n",
    "# Define ResidualGatedGCNLayer\n",
    "class ResidualGatedGCNLayer(nn.Module):\n",
    "    \"\"\"Convnet layer with gating and residual connection.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, aggregation=\"sum\"):\n",
    "        super(ResidualGatedGCNLayer, self).__init__()\n",
    "        self.node_feat = NodeFeatures(hidden_dim, aggregation)\n",
    "        self.edge_feat = EdgeFeatures(hidden_dim)\n",
    "        self.bn_node = BatchNormNode(hidden_dim)\n",
    "        self.bn_edge = BatchNormEdge(hidden_dim)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        # x: (B, V, H)\n",
    "        # e: (B, V, V, H)\n",
    "        e_new = self.edge_feat(x, e)  # (B, V, V, H)\n",
    "        edge_gate = torch.sigmoid(e_new)  # (B, V, V, H)\n",
    "        x_new = self.node_feat(x, edge_gate)  # (B, V, H)\n",
    "        e_new = self.bn_edge(e_new)  # (B, V, V, H)\n",
    "        x_new = self.bn_node(x_new)  # (B, V, H)\n",
    "        e_new = F.relu(e_new)  # (B, V, V, H)\n",
    "        x_new = F.relu(x_new)  # (B, V, H)\n",
    "        return x + x_new, e + e_new  # Residual connections\n",
    "\n",
    "# Define ResidualGatedGCNModel\n",
    "class ResidualGatedGCNModel(nn.Module):\n",
    "    \"\"\"Residual Gated GCN Model for predicting edge adjacency matrices.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(ResidualGatedGCNModel, self).__init__()\n",
    "        self.num_nodes = config.num_nodes\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.num_layers = config.num_layers\n",
    "        self.aggregation = config.aggregation\n",
    "\n",
    "        # Node coordinate embedding\n",
    "        self.nodes_coord_embedding = nn.Linear(config.node_dim, self.hidden_dim, bias=False)\n",
    "\n",
    "        # Edge value and edge type embedding\n",
    "        self.edges_values_embedding = nn.Linear(1, self.hidden_dim // 2, bias=False)\n",
    "        self.edges_embedding = nn.Embedding(config.voc_edges_in, self.hidden_dim // 2)\n",
    "\n",
    "        # GCN Layers\n",
    "        self.gcn_layers = nn.ModuleList([\n",
    "            ResidualGatedGCNLayer(self.hidden_dim, self.aggregation) for _ in range(self.num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x_edges, x_edges_values, x_nodes_coord):\n",
    "        # Embeddings\n",
    "        x = self.nodes_coord_embedding(x_nodes_coord)  # (B, V, H)\n",
    "        e_vals = self.edges_values_embedding(x_edges_values.unsqueeze(-1))  # (B, V, V, H//2)\n",
    "        e_tags = self.edges_embedding(x_edges)  # (B, V, V, H//2)\n",
    "        e = torch.cat((e_vals, e_tags), dim=-1)  # (B, V, V, H)\n",
    "\n",
    "        # GCN layers\n",
    "        for layer in self.gcn_layers:\n",
    "            x, e = layer(x, e)  # (B, V, H), (B, V, V, H)\n",
    "\n",
    "        return x, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeTokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, padding_idx):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=padding_idx)\n",
    "    def forward(self, token_seq):\n",
    "        # token_seq: (B, L) -> embedded: (B, L, d_model)\n",
    "        return self.emb(token_seq)\n",
    "\n",
    "class TransformerEdgeDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoregressive Transformer decoder that:\n",
    "      - takes node embeddings as cross-attention context\n",
    "      - decodes edge tokens in canonical order\n",
    "      - no positional encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, vocab_size=1228, nhead=4, num_layers=4, padding_idx = 1225):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.token_embedding = EdgeTokenEmbedding(vocab_size, d_model, padding_idx = padding_idx)\n",
    "        # no positional encoding\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=d_model*4,\n",
    "                                                   activation='relu',\n",
    "                                                   batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, \n",
    "                tgt_tokens,         # (B, L) teacher-forced token seq\n",
    "                node_emb,           # (B, V, H) context for cross-attn\n",
    "                tgt_mask=None,      # (L, L) for autoregressive\n",
    "                memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Returns logits over vocab for each position in tgt_tokens.\n",
    "        Shapes:\n",
    "          - tgt_tokens: (B,L)\n",
    "          - node_emb:   (B,V,H) => memory\n",
    "        \"\"\"\n",
    "        B,L = tgt_tokens.shape\n",
    "        # embed tokens\n",
    "        tgt_emb = self.token_embedding(tgt_tokens)  # (B,L,d_model)\n",
    "\n",
    "        # We'll treat node_emb as memory => shape => (B,V,H).\n",
    "        # However, TransformerDecoder expects shape (N, S, E) if batch_first=True => we pass (B,V,H).\n",
    "        # No mask for memory?\n",
    "        memory = node_emb  # (B,V,H)\n",
    "\n",
    "        # create subsequent mask for autoregressive\n",
    "        if tgt_mask is None:\n",
    "            # (L,L) with True => block\n",
    "            tgt_mask = self.generate_square_subsequent_mask(L, device=tgt_tokens.device)\n",
    "\n",
    "        out = self.decoder(tgt=tgt_emb,\n",
    "                           memory=memory,\n",
    "                           tgt_mask=tgt_mask)  # shape (B,L,d_model)\n",
    "\n",
    "        logits = self.output_projection(out) # (B,L,vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz, device):\n",
    "        # mask shape (sz, sz) => True means blocked\n",
    "        mask = torch.triu(torch.ones(sz, sz, dtype=torch.bool, device=device), diagonal=1)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNTransformerEdgeModel(nn.Module):\n",
    "    def __init__(self, config, edge_vocab_size):\n",
    "        super().__init__()\n",
    "        self.gcn = ResidualGatedGCNModel(config)\n",
    "        d_model = config.decoder_d_model\n",
    "        self.decoder = TransformerEdgeDecoder(d_model=d_model,\n",
    "                                              vocab_size=edge_vocab_size,\n",
    "                                              nhead=config.decoder_nhead,\n",
    "                                              num_layers=config.decoder_layers,\n",
    "                                              padding_idx=edge_vocab_size-3)\n",
    "        self.edge_vocab_size = edge_vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, \n",
    "                x_edges,            # (B,V,V)\n",
    "                x_edges_values,     # (B,V,V)\n",
    "                x_nodes_coord,      # (B,V,2)\n",
    "                token_seq,          # (B,L) teacher forced\n",
    "                edge_context = None):         \n",
    "        \"\"\"\n",
    "        Steps:\n",
    "          Nc: Number of edge context features per sample\n",
    "          1) GCN => node_emb (concatenated with edge context if any)=> shape (B,V+Nc,H)\n",
    "          2) decoder => logits => shape (B,L,vocab_size)\n",
    "        \"\"\"\n",
    "        # If edge_context is provided, concatenate it along dim=1\n",
    "        # Assuming edge_context has shape (B,E,H) and we want (B,V+E,H)\n",
    "        node_emb, edge_emb = self.gcn(x_edges, x_edges_values, x_nodes_coord)  # (B,V,H)\n",
    "        if edge_context is not None:\n",
    "            node_emb = torch.cat([node_emb, edge_context], dim=1)  # (B,V+E,H)\n",
    "        edge_emb_flat = edge_emb.view(edge_emb.size(0), -1, edge_emb.size(-1))\n",
    "\n",
    "        # pass to decoder\n",
    "        logits = self.decoder(token_seq, node_emb)  # (B,L,vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_context(decoder_embedding, topdown_tokens, edge_rank, N=2):\n",
    "    \"\"\"\n",
    "    Generate edge context embeddings from the top-down tokens for a batch.\n",
    "    \n",
    "    Args:\n",
    "        decoder_embedding (nn.Module): The EdgeTokenEmbedding module.\n",
    "        topdown_tokens (torch.Tensor): Tensor of shape (B, L_td) containing top-down edge tokens.\n",
    "        edge_rank (torch.Tensor): Tensor of shape (B, vocab_size) containing precomputed edge ranks.\n",
    "        N (int): Number of top tokens to select for context.\n",
    "        pad_idx (int, optional): Padding token ID to exclude.\n",
    "        start_idx (int, optional): Start token ID to exclude.\n",
    "        end_idx (int, optional): End token ID to exclude.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Edge context embeddings of shape (B, N, d_model).\n",
    "    \"\"\"\n",
    "\n",
    "    B, L = topdown_tokens.shape\n",
    "    \n",
    "    # Gather edge ranks for the tokens in topdown_tokens\n",
    "    # edge_rank has shape (B, vocab_size)\n",
    "    # topdown_tokens has shape (B, L)\n",
    "    rank_vals = edge_rank.gather(1, topdown_tokens)  # (B, L)\n",
    "    \n",
    "    # Get top N tokens based on rank_vals for each sample in the batch\n",
    "    topk_vals, topk_indices = torch.topk(rank_vals, N, dim=1, largest=True, sorted=True)  # (B, N)\n",
    "    \n",
    "    # Gather the top N tokens using the indices\n",
    "    topk_tokens = torch.gather(topdown_tokens, 1, topk_indices)  # (B, N)\n",
    "    # Embed the selected top N tokens\n",
    "    context_emb = decoder_embedding(topk_tokens)  # (B, N, d_model)\n",
    "    \n",
    "    return context_emb  # (B, N, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# 6) Training: We define a new 'train_one_epoch' that:\n",
    "#     - loads batch from the new TSPDataset\n",
    "#     - do 2-part approach: topDown + leftRight\n",
    "#     - do teacher forcing\n",
    "#     - cross entropy over each token\n",
    "#     - use the GCN for context\n",
    "#    We'll also define a normal crossEntropy ignoring class weights\n",
    "#    or if we want class weights => we can define them if we see\n",
    "#    an imbalance in edge tokens. We'll keep it simple now.\n",
    "###########################################################\n",
    "\n",
    "def sequence_cross_entropy(logits, target_seq, pad_token):\n",
    "    \"\"\"\n",
    "    logits: (B,L,vocab_size)\n",
    "    target_seq: (B,L)\n",
    "    Returns average cross-entropy\n",
    "    \"\"\"\n",
    "    B,L,_ = logits.shape\n",
    "    # flatten\n",
    "    logits_flat = logits.reshape(B*L, -1)   # (B*L, vocab_size)\n",
    "    target_flat = target_seq.reshape(B*L)   # (B*L)\n",
    "    loss = F.cross_entropy(logits_flat, target_flat, ignore_index=pad_token, reduction='mean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_logit(logits, generated_seq, start_token, end_token, pad_token, device):\n",
    "    # during evaluation if the end token is encountered, we pad all subsequent tokens\n",
    "    # this makes loss estimation more accurate\n",
    "    # Find the first occurrence of end_token in generated_td for each sample\n",
    "\n",
    "    B, _, C = logits.shape\n",
    "    #inserting start token at the beginning time step of logit\n",
    "    start_logits = torch.zeros((B, 1, C), device=device)\n",
    "    start_logits[:, 0, start_token] = 100.0  # Assign a high value to <START> token\n",
    "    logits = torch.cat([start_logits, logits], dim=1)  # (B, L+1, vocab_size)\n",
    "\n",
    "    L = logits.size(1)\n",
    "\n",
    "    mask_end = (generated_seq == end_token)  # (B, L)\n",
    "    has_end = mask_end.any(dim=1)            # (B,)\n",
    "    end_indices = mask_end.float().argmax(dim=1)  # (B,)\n",
    "\n",
    "    # Create position indices\n",
    "    \n",
    "    positions = torch.arange(L, device=device).unsqueeze(0).expand(B, L)  # (B, L)\n",
    "\n",
    "    # Create a mask for positions after the first end_token\n",
    "    mask_after_end = positions > end_indices.unsqueeze(1)  # (B, L)\n",
    "    mask_after_end = mask_after_end & has_end.unsqueeze(1)  # (B, L)\n",
    "    \n",
    "\n",
    "    # Set logits after end_token to 0\n",
    "    logits = logits.masked_fill(mask_after_end.unsqueeze(-1), 0.0)\n",
    "\n",
    "    # Set pad_idx logits to 100 where mask_after_end is True\n",
    "    logits[:, :, pad_token] = logits[:, :, pad_token].masked_fill(mask_after_end, 100.0)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_pass_train_single_epoch(model,\n",
    "                        dataloader,\n",
    "                        optimizer,\n",
    "                        config,\n",
    "                        epoch,\n",
    "                        writer=None):\n",
    "    \"\"\"\n",
    "    Example of a single training step with 2-pass approach.\n",
    "    1) Pass 1 => predict top-down edges.\n",
    "    2) Compute edge_context embeddings from pass 1's predictions.\n",
    "    3) Pass 2 => predict left-right edges using node_emb + edge_context.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_count = 0\n",
    "    start_t = time.time()\n",
    "\n",
    "    pad_token = config.special_tokens['PAD']\n",
    "    start_token = config.special_tokens['START']\n",
    "    end_token = config.special_tokens['END']\n",
    "\n",
    "    #progress = tqdm(dataloader, desc=f\"Train(Epoch {epoch})\", unit=\"batch\")\n",
    "    #for i, batch in enumerate(progress):\n",
    "    for batch in dataloader:\n",
    "        # Unpack batch\n",
    "        coords = batch['coords']         # (B,N,2)\n",
    "        coords_swapped = batch['coords_swapped']  # (B,N,2)\n",
    "        x_edges = batch['x_edges']        # (B,N,N)\n",
    "        x_edges_values = batch['x_edges_values']  # (B,N,N)\n",
    "        token_seq_td = batch['topDownTokens']  # (B,L_td)\n",
    "        token_seq_lr = batch['leftRightTokens'] # (B,L_lr)\n",
    "\n",
    "        #TODO: Think if we need to shift target sequence by 1 to the right\n",
    "        # Pass 1: top-down prediction\n",
    "        logits_td = model(\n",
    "            x_edges,\n",
    "            x_edges_values,\n",
    "\n",
    "    # Set logits after end_token\n",
    "            coords,\n",
    "            token_seq_td,\n",
    "            edge_context=None\n",
    "        )\n",
    "\n",
    "        target_seq_td = token_seq_td[:, 1:].contiguous()  # (B, L-1)\n",
    "\n",
    "        # Adjust logits_td to exclude the last time step\n",
    "        logits_td = logits_td[:, :-1, :].contiguous()    # (B, L-1, vocab_size)\n",
    "\n",
    "        # Compute loss with shifted targets\n",
    "        loss_td = sequence_cross_entropy(logits_td, target_seq_td, pad_token)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_td.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        B = coords.shape[0]\n",
    "        '''\n",
    "        #Generate edge context for pass 2\n",
    "        B = coords.shape[0]\n",
    "        real_edge_context = generate_edge_context(model.decoder.token_embedding, token_seq_td, batch['edge_rank'], N=2)\n",
    "        \n",
    "        #dummy_edge_context = torch.zeros((B, 5, config.hidden_dim), device=device)\n",
    "\n",
    "        # Pass 2: left-right prediction using edge_context from pass 1\n",
    "        logits_lr = model(\n",
    "            x_edges,\n",
    "            x_edges_values,\n",
    "            coords_swapped,\n",
    "            token_seq_lr,\n",
    "            edge_context=real_edge_context\n",
    "        )\n",
    "        \n",
    "        target_seq_lr = token_seq_lr[:, 1:].contiguous()  # (B, L-1)\n",
    "\n",
    "        # Adjust logits_td to exclude the last time step\n",
    "        logits_lr = logits_lr[:, :-1, :].contiguous()    # (B, L-1, vocab_size)\n",
    "\n",
    "        # Compute loss with shifted targets\n",
    "        loss_lr = sequence_cross_entropy(logits_lr, target_seq_lr, pad_token)\n",
    "\n",
    "\n",
    "        loss = loss_td + loss_lr\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        '''\n",
    "        running_loss += (loss_td.item())*B\n",
    "        running_count += B\n",
    "        avg_loss = running_loss/running_count\n",
    "\n",
    "        #progress.set_postfix({'loss':f\"{avg_loss:.4f}\"})\n",
    "    \n",
    "    epoch_time = time.time()-start_t\n",
    "    final_loss = running_loss/running_count\n",
    "    if writer:\n",
    "        writer.add_scalar(\"Train/Epoch_Loss\", final_loss, epoch)\n",
    "    return epoch_time, final_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_pass_evaluate(model,\n",
    "                      dataloader,\n",
    "                      optimizer,\n",
    "                      config,\n",
    "                      epoch,\n",
    "                      writer=None):\n",
    "    \"\"\"\n",
    "    Example of a single evaluation step with 2-pass approach.\n",
    "    1) Pass 1 => predict top-down edges.\n",
    "    2) Compute edge_context embeddings from pass 1's predictions.\n",
    "    3) Pass 2 => predict left-right edges using node_emb + edge_context.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_count = 0\n",
    "    start_t = time.time()\n",
    "\n",
    "    pad_token = config.special_tokens['PAD']\n",
    "    start_token = config.special_tokens['START']\n",
    "    end_token = config.special_tokens['END']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress = tqdm(dataloader, desc=f\"Eval(Epoch {epoch})\", unit=\"batch\")\n",
    "        for i, batch in enumerate(progress):\n",
    "\n",
    "            # Unpack batch\n",
    "            coords = batch['coords']                # (B,N,2)\n",
    "            coords_swapped = batch['coords_swapped'] # (B,N,2)\n",
    "            x_edges = batch['x_edges']              # (B,N,N)\n",
    "            x_edges_values = batch['x_edges_values'] # (B,N,N)\n",
    "            edge_rank = batch['edge_rank']           # (B, vocab_size)\n",
    "            token_seq_td = batch['topDownTokens']    # (B,L_td)\n",
    "            token_seq_lr = batch['leftRightTokens']  # (B,L_lr)\n",
    "\n",
    "            B = coords.shape[0]\n",
    "            # Pass 1: Predict top-down edges via greedy decoding\n",
    "            generated_td = torch.full((B, 1), start_token, dtype=torch.long, device=config.device)  # (B,1)\n",
    "            max_len_td = token_seq_td.size(1)\n",
    "            for _ in range(max_len_td-1):\n",
    "                logits_td_step = model(\n",
    "                    x_edges,\n",
    "                    x_edges_values,\n",
    "                    coords,\n",
    "                    generated_td,\n",
    "                    edge_context=None\n",
    "                )  # (B, current_L, vocab_size)\n",
    "                \n",
    "                next_token = logits_td_step[:, -1, :].argmax(dim=-1, keepdim=True)  # (B,1)\n",
    "                generated_td = torch.cat([generated_td, next_token], dim=1)  # (B, L)\n",
    "\n",
    "            print('Prediction')\n",
    "            print(generated_td)\n",
    "            print('Target')\n",
    "            print(token_seq_td)\n",
    "            \n",
    "            \n",
    "            logits_td = generate_padded_logit(logits_td_step, generated_td, start_token, end_token, pad_token, config.device)\n",
    "            loss_td = sequence_cross_entropy(logits_td, token_seq_td, pad_token)\n",
    "            '''\n",
    "            # Generate edge_context\n",
    "            real_edge_context = generate_edge_context(\n",
    "                decoder_embedding=model.module.module.token_embedding,\n",
    "                topdown_tokens=generated_td,\n",
    "                edge_rank=edge_rank,\n",
    "                N=2)  # (B, N, d_model)\n",
    "\n",
    "            generated_lr = torch.full((B, 1), start_token, dtype=torch.long, device=config.device)  # (B,1)\n",
    "            max_len_lr = token_seq_lr.size(1)\n",
    "\n",
    "            for _ in range(max_len_lr-1):\n",
    "                logits_lr_step = model(\n",
    "                    x_edges,\n",
    "                    x_edges_values,\n",
    "                    coords_swapped,\n",
    "                    generated_lr,\n",
    "                    edge_context=real_edge_context\n",
    "                )  # (B, current_L, vocab_size)\n",
    "                next_token = logits_lr_step[:, -1, :].argmax(dim=-1, keepdim=True)  # (B,1)\n",
    "                generated_lr = torch.cat([generated_lr, next_token], dim=1)  # (B, L+1)\n",
    "\n",
    "            # logits_lr is the logits from the last step\n",
    "            logits_lr = generate_padded_logit(logits_lr_step, generated_lr, start_token, end_token, pad_token, config.device)\n",
    "            loss_lr = sequence_cross_entropy(logits_lr, token_seq_lr, pad_token)\n",
    "            '''\n",
    "\n",
    "            running_loss += (loss_td).item() * B\n",
    "            running_count += B\n",
    "            avg_loss = running_loss / running_count\n",
    "\n",
    "            progress.set_postfix({'loss': f\"{avg_loss:.4f}\"})\n",
    "\n",
    "    epoch_time = time.time() - start_t\n",
    "    final_loss = running_loss / running_count\n",
    "    if writer:\n",
    "        writer.add_scalar(\"Eval/Epoch_Loss\", final_loss, epoch)\n",
    "    return epoch_time, final_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss=7.281490993499756\n",
      "Epoch 2, train loss=7.222031307220459\n",
      "Epoch 3, train loss=7.169310855865478\n",
      "Epoch 4, train loss=7.121131038665771\n",
      "Epoch 5, train loss=7.062925148010254\n",
      "Epoch 6, train loss=7.007898330688477\n",
      "Epoch 7, train loss=6.928836727142334\n",
      "Epoch 8, train loss=6.899309253692627\n",
      "Epoch 9, train loss=6.812558555603028\n",
      "Epoch 10, train loss=6.766373348236084\n",
      "Epoch 11, train loss=6.666946506500244\n",
      "Epoch 12, train loss=6.59032974243164\n",
      "Epoch 13, train loss=6.560261249542236\n",
      "Epoch 14, train loss=6.469651794433593\n",
      "Epoch 15, train loss=6.38899974822998\n",
      "Epoch 16, train loss=6.343532848358154\n",
      "Epoch 17, train loss=6.302441596984863\n",
      "Epoch 18, train loss=6.180909061431885\n",
      "Epoch 19, train loss=6.138235759735108\n",
      "Epoch 20, train loss=6.041619491577149\n",
      "Epoch 21, train loss=5.9518474578857425\n",
      "Epoch 22, train loss=5.915154552459716\n",
      "Epoch 23, train loss=5.816425418853759\n",
      "Epoch 24, train loss=5.740346431732178\n",
      "Epoch 25, train loss=5.672753047943115\n",
      "Epoch 26, train loss=5.601634693145752\n",
      "Epoch 27, train loss=5.520669078826904\n",
      "Epoch 28, train loss=5.458396053314209\n",
      "Epoch 29, train loss=5.376609992980957\n",
      "Epoch 30, train loss=5.310148429870606\n",
      "Epoch 31, train loss=5.216027164459229\n",
      "Epoch 32, train loss=5.173997020721435\n",
      "Epoch 33, train loss=5.112847900390625\n",
      "Epoch 34, train loss=5.046001338958741\n",
      "Epoch 35, train loss=4.974209213256836\n",
      "Epoch 36, train loss=4.9089373588562015\n",
      "Epoch 37, train loss=4.852183151245117\n",
      "Epoch 38, train loss=4.760578060150147\n",
      "Epoch 39, train loss=4.725462341308594\n",
      "Epoch 40, train loss=4.694494438171387\n",
      "Epoch 41, train loss=4.649687099456787\n",
      "Epoch 42, train loss=4.575157928466797\n",
      "Epoch 43, train loss=4.4982904434204105\n",
      "Epoch 44, train loss=4.439279842376709\n",
      "Epoch 45, train loss=4.435038948059082\n",
      "Epoch 46, train loss=4.397090911865234\n",
      "Epoch 47, train loss=4.331216430664062\n",
      "Epoch 48, train loss=4.302579307556153\n",
      "Epoch 49, train loss=4.215285348892212\n",
      "Epoch 50, train loss=4.201830244064331\n",
      "Epoch 51, train loss=4.181231117248535\n",
      "Epoch 52, train loss=4.174662494659424\n",
      "Epoch 53, train loss=4.112631940841675\n",
      "Epoch 54, train loss=4.086905002593994\n",
      "Epoch 55, train loss=4.083371019363403\n",
      "Epoch 56, train loss=4.04791111946106\n",
      "Epoch 57, train loss=4.029005146026611\n",
      "Epoch 58, train loss=3.997515821456909\n",
      "Epoch 59, train loss=3.9870604991912844\n",
      "Epoch 60, train loss=3.9119237422943116\n",
      "Epoch 61, train loss=3.9278733253479006\n",
      "Epoch 62, train loss=3.864344263076782\n",
      "Epoch 63, train loss=3.87430157661438\n",
      "Epoch 64, train loss=3.828732061386108\n",
      "Epoch 65, train loss=3.7971381664276125\n",
      "Epoch 66, train loss=3.7289562225341797\n",
      "Epoch 67, train loss=3.7967945098876954\n",
      "Epoch 68, train loss=3.731207036972046\n",
      "Epoch 69, train loss=3.766152858734131\n",
      "Epoch 70, train loss=3.7674725532531737\n",
      "Epoch 71, train loss=3.6713616371154787\n",
      "Epoch 72, train loss=3.6419960975646974\n",
      "Epoch 73, train loss=3.6159433841705324\n",
      "Epoch 74, train loss=3.6046653747558595\n",
      "Epoch 75, train loss=3.6166490077972413\n",
      "Epoch 76, train loss=3.5728690147399904\n",
      "Epoch 77, train loss=3.5603326320648194\n",
      "Epoch 78, train loss=3.5543994426727297\n",
      "Epoch 79, train loss=3.5125537872314454\n",
      "Epoch 80, train loss=3.49427752494812\n",
      "Epoch 81, train loss=3.4689499378204345\n",
      "Epoch 82, train loss=3.417656993865967\n",
      "Epoch 83, train loss=3.469548988342285\n",
      "Epoch 84, train loss=3.400743341445923\n",
      "Epoch 85, train loss=3.3484463691711426\n",
      "Epoch 86, train loss=3.359529209136963\n",
      "Epoch 87, train loss=3.345870351791382\n",
      "Epoch 88, train loss=3.3059720516204836\n",
      "Epoch 89, train loss=3.306799840927124\n",
      "Epoch 90, train loss=3.2804996013641357\n",
      "Epoch 91, train loss=3.2842790126800536\n",
      "Epoch 92, train loss=3.282122755050659\n",
      "Epoch 93, train loss=3.2212737083435057\n",
      "Epoch 94, train loss=3.193524169921875\n",
      "Epoch 95, train loss=3.215974712371826\n",
      "Epoch 96, train loss=3.114268445968628\n",
      "Epoch 97, train loss=3.1449482917785643\n",
      "Epoch 98, train loss=3.184481716156006\n",
      "Epoch 99, train loss=3.1048685550689696\n",
      "Epoch 100, train loss=3.15323543548584\n",
      "Epoch 101, train loss=3.011474943161011\n",
      "Epoch 102, train loss=3.0515889644622805\n",
      "Epoch 103, train loss=3.0388901233673096\n",
      "Epoch 104, train loss=2.9938005924224855\n",
      "Epoch 105, train loss=3.0018866062164307\n",
      "Epoch 106, train loss=2.9757412910461425\n",
      "Epoch 107, train loss=2.9512545585632326\n",
      "Epoch 108, train loss=2.890498971939087\n",
      "Epoch 109, train loss=2.9721554279327393\n",
      "Epoch 110, train loss=2.9032760143280028\n",
      "Epoch 111, train loss=2.8264046192169188\n",
      "Epoch 112, train loss=2.851594638824463\n",
      "Epoch 113, train loss=2.836107349395752\n",
      "Epoch 114, train loss=2.8384814262390137\n",
      "Epoch 115, train loss=2.8132237434387206\n",
      "Epoch 116, train loss=2.8274205207824705\n",
      "Epoch 117, train loss=2.774299955368042\n",
      "Epoch 118, train loss=2.7532256603240968\n",
      "Epoch 119, train loss=2.7133927822113035\n",
      "Epoch 120, train loss=2.706473445892334\n",
      "Epoch 121, train loss=2.741486692428589\n",
      "Epoch 122, train loss=2.670345401763916\n",
      "Epoch 123, train loss=2.699935865402222\n",
      "Epoch 124, train loss=2.694980335235596\n",
      "Epoch 125, train loss=2.628926134109497\n",
      "Epoch 126, train loss=2.659879541397095\n",
      "Epoch 127, train loss=2.648708391189575\n",
      "Epoch 128, train loss=2.6147979259490968\n",
      "Epoch 129, train loss=2.5412164688110352\n",
      "Epoch 130, train loss=2.57753267288208\n",
      "Epoch 131, train loss=2.5400850772857666\n",
      "Epoch 132, train loss=2.529290771484375\n",
      "Epoch 133, train loss=2.5477985382080077\n",
      "Epoch 134, train loss=2.544286823272705\n",
      "Epoch 135, train loss=2.519020748138428\n",
      "Epoch 136, train loss=2.4300891876220705\n",
      "Epoch 137, train loss=2.4964303493499758\n",
      "Epoch 138, train loss=2.4468898296356203\n",
      "Epoch 139, train loss=2.446565055847168\n",
      "Epoch 140, train loss=2.397103452682495\n",
      "Epoch 141, train loss=2.4455850601196287\n",
      "Epoch 142, train loss=2.3737907886505125\n",
      "Epoch 143, train loss=2.394602108001709\n",
      "Epoch 144, train loss=2.3522261142730714\n",
      "Epoch 145, train loss=2.4032822132110594\n",
      "Epoch 146, train loss=2.3541190147399904\n",
      "Epoch 147, train loss=2.3515549659729005\n",
      "Epoch 148, train loss=2.3095203399658204\n",
      "Epoch 149, train loss=2.3063610076904295\n",
      "Epoch 150, train loss=2.285757637023926\n",
      "Epoch 151, train loss=2.2754824638366697\n",
      "Epoch 152, train loss=2.1981873750686645\n",
      "Epoch 153, train loss=2.2485387325286865\n",
      "Epoch 154, train loss=2.2616623401641847\n",
      "Epoch 155, train loss=2.219258213043213\n",
      "Epoch 156, train loss=2.22430214881897\n",
      "Epoch 157, train loss=2.222136974334717\n",
      "Epoch 158, train loss=2.1784579753875732\n",
      "Epoch 159, train loss=2.2007032871246337\n",
      "Epoch 160, train loss=2.1984453439712524\n",
      "Epoch 161, train loss=2.173048162460327\n",
      "Epoch 162, train loss=2.1289912939071653\n",
      "Epoch 163, train loss=2.19501953125\n",
      "Epoch 164, train loss=2.116893482208252\n",
      "Epoch 165, train loss=2.05645637512207\n",
      "Epoch 166, train loss=2.053646755218506\n",
      "Epoch 167, train loss=2.05447940826416\n",
      "Epoch 168, train loss=2.117830991744995\n",
      "Epoch 169, train loss=2.0523765087127686\n",
      "Epoch 170, train loss=2.0425978660583497\n",
      "Epoch 171, train loss=2.016240930557251\n",
      "Epoch 172, train loss=1.990839385986328\n",
      "Epoch 173, train loss=1.9653273582458497\n",
      "Epoch 174, train loss=1.951261615753174\n",
      "Epoch 175, train loss=1.9396229267120362\n",
      "Epoch 176, train loss=1.9536470413208007\n",
      "Epoch 177, train loss=1.9527824878692628\n",
      "Epoch 178, train loss=1.943109107017517\n",
      "Epoch 179, train loss=1.9314439058303834\n",
      "Epoch 180, train loss=1.9071730375289917\n",
      "Epoch 181, train loss=1.881896162033081\n",
      "Epoch 182, train loss=1.9049867630004882\n",
      "Epoch 183, train loss=1.9033217430114746\n",
      "Epoch 184, train loss=1.8627485275268554\n",
      "Epoch 185, train loss=1.8447019100189208\n",
      "Epoch 186, train loss=1.86331205368042\n",
      "Epoch 187, train loss=1.8300426006317139\n",
      "Epoch 188, train loss=1.8171183824539185\n",
      "Epoch 189, train loss=1.8113355398178101\n",
      "Epoch 190, train loss=1.7751732110977172\n",
      "Epoch 191, train loss=1.7865182399749755\n",
      "Epoch 192, train loss=1.7824939966201783\n",
      "Epoch 193, train loss=1.7714792966842652\n",
      "Epoch 194, train loss=1.751870322227478\n",
      "Epoch 195, train loss=1.7516799926757813\n",
      "Epoch 196, train loss=1.7317257165908813\n",
      "Epoch 197, train loss=1.7659336566925048\n",
      "Epoch 198, train loss=1.6947494745254517\n",
      "Epoch 199, train loss=1.699442195892334\n",
      "Epoch 200, train loss=1.7121809005737305\n",
      "Epoch 201, train loss=1.7263872623443604\n",
      "Epoch 202, train loss=1.662093162536621\n",
      "Epoch 203, train loss=1.661203670501709\n",
      "Epoch 204, train loss=1.6652048587799073\n",
      "Epoch 205, train loss=1.6777858018875123\n",
      "Epoch 206, train loss=1.6349631786346435\n",
      "Epoch 207, train loss=1.60917067527771\n",
      "Epoch 208, train loss=1.5943114042282105\n",
      "Epoch 209, train loss=1.6417815446853639\n",
      "Epoch 210, train loss=1.596809411048889\n",
      "Epoch 211, train loss=1.59231858253479\n",
      "Epoch 212, train loss=1.578963303565979\n",
      "Epoch 213, train loss=1.6296378374099731\n",
      "Epoch 214, train loss=1.591104793548584\n",
      "Epoch 215, train loss=1.5425152063369751\n",
      "Epoch 216, train loss=1.575973677635193\n",
      "Epoch 217, train loss=1.5403460741043091\n",
      "Epoch 218, train loss=1.5720702886581421\n",
      "Epoch 219, train loss=1.5599369525909423\n",
      "Epoch 220, train loss=1.496241283416748\n",
      "Epoch 221, train loss=1.4945909976959229\n",
      "Epoch 222, train loss=1.528714370727539\n",
      "Epoch 223, train loss=1.4948413610458373\n",
      "Epoch 224, train loss=1.4974294662475587\n",
      "Epoch 225, train loss=1.4487569808959961\n",
      "Epoch 226, train loss=1.4490283250808715\n",
      "Epoch 227, train loss=1.440919542312622\n",
      "Epoch 228, train loss=1.4219974756240845\n",
      "Epoch 229, train loss=1.4638798475265502\n",
      "Epoch 230, train loss=1.4627958059310913\n",
      "Epoch 231, train loss=1.43757905960083\n",
      "Epoch 232, train loss=1.4143076896667481\n",
      "Epoch 233, train loss=1.4239388227462768\n",
      "Epoch 234, train loss=1.408931827545166\n",
      "Epoch 235, train loss=1.4044569492340089\n",
      "Epoch 236, train loss=1.3833602905273437\n",
      "Epoch 237, train loss=1.3957542896270752\n",
      "Epoch 238, train loss=1.3323792219161987\n",
      "Epoch 239, train loss=1.4022197246551513\n",
      "Epoch 240, train loss=1.3400061130523682\n",
      "Epoch 241, train loss=1.3422658443450928\n",
      "Epoch 242, train loss=1.3318745374679566\n",
      "Epoch 243, train loss=1.3240851163864136\n",
      "Epoch 244, train loss=1.3112576961517335\n",
      "Epoch 245, train loss=1.2968852043151855\n",
      "Epoch 246, train loss=1.302204966545105\n",
      "Epoch 247, train loss=1.3224036693572998\n",
      "Epoch 248, train loss=1.3103589773178101\n",
      "Epoch 249, train loss=1.3104326009750367\n",
      "Epoch 250, train loss=1.2645424365997315\n",
      "Epoch 251, train loss=1.300562047958374\n",
      "Epoch 252, train loss=1.2705556631088257\n",
      "Epoch 253, train loss=1.2691821575164794\n",
      "Epoch 254, train loss=1.2458239793777466\n",
      "Epoch 255, train loss=1.254287099838257\n",
      "Epoch 256, train loss=1.198173213005066\n",
      "Epoch 257, train loss=1.2450262308120728\n",
      "Epoch 258, train loss=1.2315649032592773\n",
      "Epoch 259, train loss=1.2323065757751466\n",
      "Epoch 260, train loss=1.25411057472229\n",
      "Epoch 261, train loss=1.2046400547027587\n",
      "Epoch 262, train loss=1.1823153495788574\n",
      "Epoch 263, train loss=1.1929447412490846\n",
      "Epoch 264, train loss=1.1653773069381714\n",
      "Epoch 265, train loss=1.1801717281341553\n",
      "Epoch 266, train loss=1.1718868494033814\n",
      "Epoch 267, train loss=1.196840524673462\n",
      "Epoch 268, train loss=1.1529196500778198\n",
      "Epoch 269, train loss=1.110227370262146\n",
      "Epoch 270, train loss=1.142908763885498\n",
      "Epoch 271, train loss=1.1548819065093994\n",
      "Epoch 272, train loss=1.1250720024108887\n",
      "Epoch 273, train loss=1.1253559827804565\n",
      "Epoch 274, train loss=1.14252028465271\n",
      "Epoch 275, train loss=1.1320226788520813\n",
      "Epoch 276, train loss=1.0776496171951293\n",
      "Epoch 277, train loss=1.118844485282898\n",
      "Epoch 278, train loss=1.0712820291519165\n",
      "Epoch 279, train loss=1.0846523761749267\n",
      "Epoch 280, train loss=1.0984848976135253\n",
      "Epoch 281, train loss=1.078075647354126\n",
      "Epoch 282, train loss=1.1140987396240234\n",
      "Epoch 283, train loss=1.0435124516487122\n",
      "Epoch 284, train loss=1.0670079469680787\n",
      "Epoch 285, train loss=1.0962514638900758\n",
      "Epoch 286, train loss=1.031735098361969\n",
      "Epoch 287, train loss=1.0465150594711303\n",
      "Epoch 288, train loss=1.0240229845046998\n",
      "Epoch 289, train loss=1.007536494731903\n",
      "Epoch 290, train loss=1.0653387546539306\n",
      "Epoch 291, train loss=0.9906551957130432\n",
      "Epoch 292, train loss=1.0391762256622314\n",
      "Epoch 293, train loss=0.9662864565849304\n",
      "Epoch 294, train loss=0.9832459092140198\n",
      "Epoch 295, train loss=0.9876486182212829\n",
      "Epoch 296, train loss=0.965130603313446\n",
      "Epoch 297, train loss=1.019069004058838\n",
      "Epoch 298, train loss=0.9790575265884399\n",
      "Epoch 299, train loss=0.9556276321411132\n",
      "Epoch 300, train loss=0.980491304397583\n",
      "Epoch 301, train loss=0.9235170483589172\n",
      "Epoch 302, train loss=0.9610455989837646\n",
      "Epoch 303, train loss=0.9422386765480042\n",
      "Epoch 304, train loss=0.9174866914749146\n",
      "Epoch 305, train loss=0.913856303691864\n",
      "Epoch 306, train loss=0.9356334209442139\n",
      "Epoch 307, train loss=0.9384074687957764\n",
      "Epoch 308, train loss=0.9406031727790832\n",
      "Epoch 309, train loss=0.9253667116165161\n",
      "Epoch 310, train loss=0.922446858882904\n",
      "Epoch 311, train loss=0.9078622817993164\n",
      "Epoch 312, train loss=0.917985713481903\n",
      "Epoch 313, train loss=0.9301545739173889\n",
      "Epoch 314, train loss=0.8800082802772522\n",
      "Epoch 315, train loss=0.8846954107284546\n",
      "Epoch 316, train loss=0.9098177313804626\n",
      "Epoch 317, train loss=0.9262772560119629\n",
      "Epoch 318, train loss=0.9071121335029602\n",
      "Epoch 319, train loss=0.8509194254875183\n",
      "Epoch 320, train loss=0.8460860848426819\n",
      "Epoch 321, train loss=0.8944628715515137\n",
      "Epoch 322, train loss=0.8654533743858337\n",
      "Epoch 323, train loss=0.8716342210769653\n",
      "Epoch 324, train loss=0.8478302121162414\n",
      "Epoch 325, train loss=0.7989359855651855\n",
      "Epoch 326, train loss=0.8731094360351562\n",
      "Epoch 327, train loss=0.8328843712806702\n",
      "Epoch 328, train loss=0.8370417594909668\n",
      "Epoch 329, train loss=0.8745995044708252\n",
      "Epoch 330, train loss=0.7635135054588318\n",
      "Epoch 331, train loss=0.8338613748550415\n",
      "Epoch 332, train loss=0.8325332045555115\n",
      "Epoch 333, train loss=0.8225017070770264\n",
      "Epoch 334, train loss=0.8716915130615235\n",
      "Epoch 335, train loss=0.8112890958786011\n",
      "Epoch 336, train loss=0.8328132390975952\n",
      "Epoch 337, train loss=0.8048414349555969\n",
      "Epoch 338, train loss=0.8589512944221497\n",
      "Epoch 339, train loss=0.7745959997177124\n",
      "Epoch 340, train loss=0.781385314464569\n",
      "Epoch 341, train loss=0.7382524967193603\n",
      "Epoch 342, train loss=0.7994157433509826\n",
      "Epoch 343, train loss=0.7228276491165161\n",
      "Epoch 344, train loss=0.7790160298347473\n",
      "Epoch 345, train loss=0.776162612438202\n",
      "Epoch 346, train loss=0.7852046608924865\n",
      "Epoch 347, train loss=0.7322235822677612\n",
      "Epoch 348, train loss=0.766397750377655\n",
      "Epoch 349, train loss=0.7897596001625061\n",
      "Epoch 350, train loss=0.8412792921066284\n",
      "Epoch 351, train loss=0.7570672154426574\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m opt_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model_auto\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1500\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     et, lss \u001b[38;5;241m=\u001b[39m \u001b[43mtwo_pass_train_single_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_auto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdummy_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg_autoreg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m _, val_lss \u001b[38;5;241m=\u001b[39m two_pass_evaluate(\n\u001b[1;32m     37\u001b[0m         model_auto, \n\u001b[1;32m     38\u001b[0m         dummy_val_loader, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m         writer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[32], line 54\u001b[0m, in \u001b[0;36mtwo_pass_train_single_epoch\u001b[0;34m(model, dataloader, optimizer, config, epoch, writer)\u001b[0m\n\u001b[1;32m     51\u001b[0m loss_td \u001b[38;5;241m=\u001b[39m sequence_cross_entropy(logits_td, target_seq_td, pad_token)\n\u001b[1;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 54\u001b[0m \u001b[43mloss_td\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     57\u001b[0m B \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/macising/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/macising/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/macising/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "edge2tok, tok2edge = build_global_vocabulary(50) \n",
    "dummy_dataset = TSPDataset(\"tsp-data/tsp50_micro.txt\", num_nodes=50, edge_to_token=edge2tok, token_to_edge=tok2edge, device=device)\n",
    "collate_fn = partial(collate_fn, edge_to_token=edge2tok)\n",
    "dummy_loader = DataLoader(dummy_dataset, batch_size=4, shuffle=True, drop_last=False, collate_fn=collate_fn)\n",
    "dummy_val_loader = DataLoader(dummy_dataset, batch_size=1, shuffle=False, drop_last=False, collate_fn=collate_fn)\n",
    "\n",
    "cfg_autoreg = DotDict({\n",
    "    'num_nodes':50,\n",
    "    'hidden_dim':8,\n",
    "    'num_layers':3,\n",
    "    'aggregation':'mean',\n",
    "    'node_dim':2,\n",
    "    'voc_edges_in':3,   \n",
    "    'decoder_d_model':8,\n",
    "    'decoder_nhead':2,\n",
    "    'decoder_layers':1,\n",
    "    'special_tokens': {'PAD': edge2tok['<PAD>'], 'START': edge2tok['<START>'], 'END': edge2tok['<END>']},\n",
    "    'device': device\n",
    "})\n",
    "\n",
    "model_auto = GCNTransformerEdgeModel(cfg_autoreg, edge_vocab_size=len(edge2tok)).to(device)\n",
    "#model_auto = nn.DataParallel(model_auto)\n",
    "\n",
    "opt_ = torch.optim.Adam(model_auto.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, 1500):\n",
    "    et, lss = two_pass_train_single_epoch(\n",
    "        model_auto, \n",
    "        dummy_loader, \n",
    "        opt_, \n",
    "        cfg_autoreg, \n",
    "        epoch=epoch, \n",
    "        writer=None\n",
    "    )\n",
    "    print(f\"Epoch {epoch}, train loss={lss}\")\n",
    "_, val_lss = two_pass_evaluate(\n",
    "        model_auto, \n",
    "        dummy_val_loader, \n",
    "        opt_, \n",
    "        cfg_autoreg, \n",
    "        epoch=epoch, \n",
    "        writer=None\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macising",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
